<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Beyond Fact Retrieval: Episodic Memory for RAG with GSW</title>
    <link rel="stylesheet" href="assets/css/style.css">
</head>
<body>
    <article>
        <header>
            <h1>Beyond Fact Retrieval:<br>Episodic Memory for RAG with<br>Generative Semantic Workspaces</h1>

            <div class="authors">
                <p><a href="#">Shreyas Rajesh</a>, <a href="#">Pavan Holur</a>, <a href="#">Chenda Duan</a>, <a href="#">David Chong</a>, <a href="#">Vwani Roychowdhury</a></p>
                <p class="affiliation">University of California, Los Angeles</p>
            </div>

            <div class="venue">
                <p><strong>AAAI 2026</strong> <span class="oral">Oral</span></p>
                <p><strong>NeurIPS 2025 Workshop on Language Agents and World Models</strong> <span class="spotlight">‚≠ê Spotlight</span></p>
            </div>

            <div class="links">
                <a href="https://arxiv.org/abs/2511.07587" class="btn" target="_blank">üìÑ Paper</a>
            </div>
        </header>

        <section id="abstract">
            <h3>TL;DR</h3>
            <p class="tldr">
                <strong>GSW achieves state-of-the-art episodic memory performance</strong> with an F1-score of <strong>0.85</strong> on EpBench,
                outperforming structured RAG baselines by up to <strong>20% in recall</strong> while reducing context tokens by <strong>51%</strong>.
            </p>
        </section>

        <section id="motivation">
            <h3>Motivation</h3>
            <p>
                Large Language Models face fundamental challenges with long-context reasoning. Current RAG solutions‚Äîfrom semantic embeddings to
                knowledge graphs‚Äîare designed for <strong>fact retrieval</strong> but fail to build the <strong>space-time-anchored narrative
                representations</strong> needed for tracking entities through evolving situations.
            </p>
            <p>
                <strong>The vast majority of texts are not lists of facts but narratives of evolving real-world situations.</strong>
                Crime reports, political briefings, corporate filings, and news coverage all describe <em>actors</em> that adopt <em>roles</em>
                and transition through <em>states</em> while interacting across <em>space and time</em>.
            </p>
        </section>

        <section id="approach">
            <h3>Our Approach: Generative Semantic Workspaces (GSW)</h3>

            <div class="figure">
                <img src="figures/motivation.png" alt="Brain-inspired GSW architecture">
                <p class="caption">
                    <strong>Brain-Inspired Design:</strong> GSW mirrors the neocortical-hippocampal architecture. The <em>Operator</em>
                    (neocortex) extracts semantic roles and states. The <em>Reconciler</em> (hippocampus) binds them into coherent
                    spatiotemporal sequences.
                </p>
            </div>

            <p>
                GSW is a <strong>neuro-inspired generative memory framework</strong> that builds structured, interpretable representations
                of evolving situations. It comprises two core components:
            </p>

            <div class="component-grid">
                <div class="component">
                    <h4>üîç Operator</h4>
                    <p>Maps incoming text to intermediate semantic structures:</p>
                    <ul>
                        <li><strong>Actors & Entities:</strong> People, places, objects, times</li>
                        <li><strong>Roles:</strong> Situation-relevant descriptors</li>
                        <li><strong>States:</strong> Evolving conditions of actors</li>
                        <li><strong>Verbs & Valences:</strong> Actions and their arguments</li>
                        <li><strong>Spatio-Temporal Links:</strong> Shared locations/times</li>
                        <li><strong>Forward-Falling Questions:</strong> Predicted developments</li>
                    </ul>
                </div>

                <div class="component">
                    <h4>üîÑ Reconciler</h4>
                    <p>Integrates semantic structures into a persistent workspace:</p>
                    <ul>
                        <li><strong>Entity Resolution:</strong> Resolves entity matches across evolving narratives.</li>
                        <li><strong>Actor States:</strong> Tracks actor states over time.</li>
                        <li><strong>Spatio-Temporal Coherence:</strong> Enforces consistency and grounds actors to the right space and time.</li>
                        <li><strong>Predictive Questions:</strong> Resolves unanswered predictive questions.</li>
                    </ul>
                </div>
            </div>

            <div class="figure figure-wide">
                <img src="figures/GSW_pipeline-2.png" alt="GSW Pipeline">
                <p class="caption">
                    <strong>End-to-End Pipeline:</strong> Documents are chunked and processed by the Operator to create local semantic graphs.
                    The Reconciler integrates these into a unified global memory. At query time, entity-specific summaries are retrieved,
                    re-ranked, and passed to an LLM.
                </p>
            </div>
        </section>

        <section id="results">
            <h3>Results</h3>

            <div class="results-highlight">
                <div class="metric-card">
                    <div class="metric-value">0.850</div>
                    <div class="metric-label">F1-Score</div>
                    <div class="metric-desc">EpBench-200</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">0.773</div>
                    <div class="metric-label">F1-Score</div>
                    <div class="metric-desc">EpBench-2000 (10x scale)</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">+20%</div>
                    <div class="metric-label">Recall</div>
                    <div class="metric-desc">vs. HippoRAG2 (next best)</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">51%</div>
                    <div class="metric-label">Token Reduction</div>
                    <div class="metric-desc">vs. GraphRAG (next best)</div>
                </div>
            </div>

            <h4>Performance on EpBench-200 (F1-Score by Query Complexity)</h4>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>0 Cues</th>
                            <th>1 Cue</th>
                            <th>2 Cues</th>
                            <th>3-5 Cues</th>
                            <th>6+ Cues</th>
                            <th>Overall</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="highlight-row">
                            <td><strong>GSW (Ours)</strong></td>
                            <td><strong>0.978</strong></td>
                            <td>0.745</td>
                            <td><strong>0.806</strong></td>
                            <td><strong>0.867</strong></td>
                            <td><strong>0.834</strong></td>
                            <td><strong>0.850</strong></td>
                        </tr>
                        <tr>
                            <td>HippoRAG2</td>
                            <td>0.828</td>
                            <td>0.675</td>
                            <td>0.762</td>
                            <td>0.755</td>
                            <td>0.746</td>
                            <td>0.753</td>
                        </tr>
                        <tr>
                            <td>Embedding RAG</td>
                            <td>0.906</td>
                            <td>0.727</td>
                            <td>0.724</td>
                            <td>0.744</td>
                            <td>0.678</td>
                            <td>0.770</td>
                        </tr>
                        <tr>
                            <td>GraphRAG</td>
                            <td>0.950</td>
                            <td>0.625</td>
                            <td>0.624</td>
                            <td>0.658</td>
                            <td>0.607</td>
                            <td>0.714</td>
                        </tr>
                        <tr>
                            <td>LightRAG</td>
                            <td>0.944</td>
                            <td>0.593</td>
                            <td>0.587</td>
                            <td>0.578</td>
                            <td>0.560</td>
                            <td>0.677</td>
                        </tr>
                        <tr>
                            <td>Vanilla LLM</td>
                            <td>0.883</td>
                            <td>0.709</td>
                            <td>0.582</td>
                            <td>0.484</td>
                            <td>0.323</td>
                            <td>0.642</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h4>Token Efficiency</h4>
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Avg. Tokens/Query</th>
                            <th>Avg. Cost/Query</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="highlight-row">
                            <td><strong>GSW (Ours)</strong></td>
                            <td><strong>~3,587</strong></td>
                            <td><strong>~$0.0090</strong></td>
                        </tr>
                        <tr>
                            <td>GraphRAG</td>
                            <td>~7,340</td>
                            <td>~$0.0184</td>
                        </tr>
                        <tr>
                            <td>Embedding RAG</td>
                            <td>~8,771</td>
                            <td>~$0.0219</td>
                        </tr>
                        <tr>
                            <td>HippoRAG2</td>
                            <td>~8,771</td>
                            <td>~$0.0219</td>
                        </tr>
                        <tr>
                            <td>LightRAG</td>
                            <td>~40,476</td>
                            <td>~$0.1012</td>
                        </tr>
                        <tr>
                            <td>Vanilla LLM</td>
                            <td>~101,120</td>
                            <td>~$0.2528</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p>
                <strong>Key Insight:</strong> GSW's entity-specific summaries provide targeted, query-relevant information‚Äîreducing
                hallucinations and drastically cutting inference costs.
            </p>
        </section>

        <section id="code">
            <h3>Code</h3>
            <p>
                <strong>Coming soon.</strong>
            </p>
        </section>

        <section id="citation">
            <h3>Cite</h3>
            <p>If you find our work useful, kindly cite our paper:</p>
            <pre><code>@misc{rajesh2025factretrievalepisodicmemory,
  title={Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces},
  author={Shreyas Rajesh and Pavan Holur and Chenda Duan and David Chong and Vwani Roychowdhury},
  year={2025},
  eprint={2511.07587},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2511.07587}
}</code></pre>
        </section>

        <footer>
            <p>¬© 2025 UCLA | <a href="https://arxiv.org/abs/2511.07587" target="_blank">ArXiv</a> | Code (Coming Soon)</p>
        </footer>
    </article>
</body>
</html>
